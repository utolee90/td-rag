import argparse
import requests, jsonlines
from newspaper import Article
import datetime
from retrieval_utils.tools import read_jsonl
import json
import datetime
import concurrent.futures
import time
from functools import lru_cache
import urllib3
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# urllib3 ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def run_gcs(key,
           engine,
           in_file,
           out_file,
           use_evidence=False,
           max_workers=4  # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ ì¶”ê°€
          ):
    questions = read_jsonl(in_file)
    outputs = []
    
    print(f"Processing {len(questions)} questions with {max_workers} workers...")
    start_time = time.time()
    
    for question in questions:
        if use_evidence:
            search_result = search(question["evidence"], key, engine)
        else:
            search_result = search(question["question_sentence"], key, engine)
        search_time = datetime.datetime.now(datetime.timezone.utc).strftime("%Y/%m/%d/%H:%M")
        
        # ë³‘ë ¬ë¡œ ê¸°ì‚¬ íŒŒì‹± ì²˜ë¦¬
        search_result = parse_articles_parallel(search_result, max_workers)
        
        output = {"question_id": question["question_id"], "search_time": search_time, "search_result": search_result}
        outputs.append(output)
    
    processing_time = time.time() - start_time
    print(f"âœ… GCS processing completed in {processing_time:.2f} seconds")
    
    return outputs

def search(query, key, engine, top_k=10, timeout=10):
    """Google Custom Search API í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒ ë° ì—ëŸ¬ ì²˜ë¦¬ ê°œì„ )"""
    url = f"https://www.googleapis.com/customsearch/v1?key={key}&cx={engine}&q={query}&start=1"
    
    try:
        response = requests.get(url, timeout=timeout)
        response.raise_for_status()  # HTTP ì—ëŸ¬ ì²´í¬
        data = response.json()
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ ê²€ìƒ‰ API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        return []
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        return []
    
    search_items = data.get("items", [])
    results = []
    
    for i, search_item in enumerate(search_items):
        try:
            long_description = search_item["pagemap"]["metatags"][0]["og:description"]
        except KeyError:
            long_description = "N/A"
            
        doc_id = f'search_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_{query[:25]}_{i}'
        title = search_item.get("title", "Unknown Title")
        snippet = search_item.get("snippet", "")
        html_snippet = search_item.get("htmlSnippet", "")
        link = search_item.get("link")
        
        if link:  # URLì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
            result = {"url": link, "title": title, "doc_id": doc_id}
            results.append(result)
            
        if len(results) >= top_k:
            break

    print(f"ğŸ” ê²€ìƒ‰ ì™„ë£Œ: '{query[:30]}...' - {len(results)}ê°œ ê²°ê³¼")
    return results

@lru_cache(maxsize=128)  # URLë³„ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ íŒŒì‹± ë°©ì§€
def parse_article(url, timeout=15):
    """ê¸°ì‚¬ íŒŒì‹± (ìºì‹± ë° íƒ€ì„ì•„ì›ƒ ì ìš©, ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬)"""
    try:
        # ê°•ë ¥í•œ ì„¸ì…˜ ì‚¬ìš©
        session = get_session()
        
        # Article ê°ì²´ ìƒì„± (ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •)
        article = Article(url)
        
        # requests_sessionì„ ì„¤ì •í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
        if hasattr(article, 'set_session'):
            article.set_session(session)
        elif hasattr(article, 'session'):
            article.session = session
            
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •ì„ ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„
        try:
            article.download()
        except Exception as download_error:
            print(f"âš ï¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ëŒ€ì²´ ë°©ë²• ì‹œë„: {download_error}")
            # ì§ì ‘ requestsë¡œ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            response = session.get(url, timeout=timeout)
            response.raise_for_status()
            article.set_html(response.text)
        
        article.parse()
        
        text = article.text or ""
        authors = article.authors or []
        
        # Wikipedia íŠ¹ë³„ ì²˜ë¦¬
        if 'wikipedia' in url.lower():
            authors = ["Wikipedia"]
        
        publish_date = article.publish_date
        if publish_date is not None:
            publish_date = publish_date.strftime("%Y/%m/%d")
        else:
            publish_date = "2018/12/31"
            
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹¤ë¥¸ ì†ì„±ë“¤ ì‹œë„
        if len(text.strip()) < 50:
            fallback_text = ""
            
            # ì•ˆì „í•˜ê²Œ ë‹¤ì–‘í•œ ì†ì„±ë“¤ ì‹œë„
            for attr in ['meta_description', 'summary', 'meta_data']:
                try:
                    value = getattr(article, attr, None)
                    if value:
                        if isinstance(value, dict):
                            fallback_text = value.get('description', '') or value.get('summary', '')
                        else:
                            fallback_text = str(value)
                        if fallback_text.strip():
                            break
                except:
                    continue
            
            text = fallback_text or text
            
        return text, authors, publish_date
        
    except Exception as e:
        print(f"âš ï¸ ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨ ({url}): {str(e)}")
        return "", [], "2018/12/31"

def parse_articles_parallel(search_result, max_workers=4):
    """ë³‘ë ¬ë¡œ ê¸°ì‚¬ë“¤ì„ íŒŒì‹±í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ"""
    if not search_result:
        return search_result
    
    def parse_single_article(article):
        """ë‹¨ì¼ ê¸°ì‚¬ íŒŒì‹± (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        try:
            article["text"], article["authors"], article["publish_date"] = parse_article(article["url"])
            return article
        except Exception as e:
            print(f'\nURL íŒŒì‹± ì‹¤íŒ¨: {article["url"]} - {str(e)}\n')
            # ì‹¤íŒ¨í•œ ê¸°ì‚¬ëŠ” ë¹ˆ ë°ì´í„°ë¡œ ì²˜ë¦¬
            article["text"], article["authors"], article["publish_date"] = "", [], "2018/12/31"
            return article
    
    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê¸°ì‚¬ë“¤ íŒŒì‹±
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        parsed_articles = list(executor.map(parse_single_article, search_result))
    
    # ìœ íš¨í•œ ê¸°ì‚¬ë“¤ë§Œ í•„í„°ë§ (í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²ƒë“¤)
    valid_articles = [article for article in parsed_articles if article["text"].strip()]
    
    print(f"âœ… íŒŒì‹± ì™„ë£Œ: {len(valid_articles)}/{len(search_result)} ê¸°ì‚¬ ì„±ê³µ")
    return valid_articles

def clear_parse_cache():
    """íŒŒì‹± ìºì‹œ í´ë¦¬ì–´ ë° ì„¸ì…˜ ì •ë¦¬ (ë©”ëª¨ë¦¬ ê´€ë¦¬ìš©)"""
    global _session
    
    parse_article.cache_clear()
    
    # ì„¸ì…˜ë„ ì •ë¦¬
    if _session:
        _session.close()
        _session = None
        
    print("âœ… ê¸°ì‚¬ íŒŒì‹± ìºì‹œì™€ ì„¸ì…˜ì´ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤.")

def get_cache_info():
    """ìºì‹œ ì‚¬ìš© í†µê³„ ë°˜í™˜"""
    cache_info = parse_article.cache_info()
    print(f"ğŸ“Š ìºì‹œ í†µê³„ - íˆíŠ¸: {cache_info.hits}, ë¯¸ìŠ¤: {cache_info.misses}, í¬ê¸°: {cache_info.currsize}/{cache_info.maxsize}")
    return cache_info

def create_robust_session(timeout=15):
    """ê°•ë ¥í•œ HTTP ì„¸ì…˜ ìƒì„± (ì¬ì‹œë„ ë° íƒ€ì„ì•„ì›ƒ ì„¤ì •)"""
    session = requests.Session()
    
    # ì¬ì‹œë„ ì „ëµ ì„¤ì •
    retry_strategy = Retry(
        total=3,
        status_forcelist=[429, 500, 502, 503, 504],
        backoff_factor=1
    )
    
    # HTTP ì–´ëŒ‘í„°ì— ì¬ì‹œë„ ì „ëµ ì ìš©
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    # ê¸°ë³¸ í—¤ë” ì„¤ì •
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })
    
    return session

# ì „ì—­ ì„¸ì…˜ ì¸ìŠ¤í„´ìŠ¤
_session = None

def get_session():
    """ì „ì—­ ì„¸ì…˜ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _session
    if _session is None:
        _session = create_robust_session()
    return _session
